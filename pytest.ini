"""
Pytest Configuration for AI-Powered 5G Open RAN Optimizer
=========================================================

This configuration provides comprehensive testing setup with fixtures,
markers, and custom plugins for the 5G optimization platform.
"""

import asyncio
import os
import tempfile
from pathlib import Path
from typing import Generator, Dict, Any

import pytest
import numpy as np
import pandas as pd
from unittest.mock import Mock, AsyncMock

# Test configuration
pytest_plugins = [
    "pytest_asyncio",
    "pytest_mock",
    "pytest_benchmark",
    "pytest_cov",
    "pytest_html",
    "pytest_xdist"
]

# Custom markers for test categorization
def pytest_configure(config):
    """Configure pytest with custom markers"""
    config.addinivalue_line(
        "markers", "unit: Unit tests for individual functions"
    )
    config.addinivalue_line(
        "markers", "integration: Integration tests for component interaction"
    )
    config.addinivalue_line(
        "markers", "performance: Performance and benchmark tests"
    )
    config.addinivalue_line(
        "markers", "cognitive: Tests for cognitive intelligence engine"
    )
    config.addinivalue_line(
        "markers", "edge_ai: Tests for edge AI components"
    )
    config.addinivalue_line(
        "markers", "security: Tests for security AI components"
    )
    config.addinivalue_line(
        "markers", "optimization: Tests for network optimization algorithms"
    )
    config.addinivalue_line(
        "markers", "prediction: Tests for predictive analytics"
    )
    config.addinivalue_line(
        "markers", "quantum: Tests for quantum-inspired algorithms"
    )
    config.addinivalue_line(
        "markers", "neuromorphic: Tests for neuromorphic computing"
    )
    config.addinivalue_line(
        "markers", "api: API endpoint tests"
    )
    config.addinivalue_line(
        "markers", "azure: Azure cloud integration tests"
    )
    config.addinivalue_line(
        "markers", "slow: Slow-running tests (>10 seconds)"
    )
    config.addinivalue_line(
        "markers", "regression: Regression tests for bug fixes"
    )

# Test collection configuration
collect_ignore = [
    "env",
    "venv", 
    "myenv",
    "network_optimization_env",
    "network_optimizer_env",
    "__pycache__"
]

# Pytest options
addopts = [
    "--strict-markers",
    "--strict-config", 
    "--verbose",
    "--tb=short",
    "--cov=src",
    "--cov-report=html:htmlcov",
    "--cov-report=xml:coverage.xml",
    "--cov-report=term-missing",
    "--cov-fail-under=80",
    "--html=reports/pytest_report.html",
    "--self-contained-html",
    "--junitxml=reports/junit.xml"
]

# Test discovery patterns
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]

# Minimum version requirements
minversion = "7.0"

# Async test configuration
asyncio_mode = "auto"

# Performance test configuration
benchmark_min_rounds = 5
benchmark_max_time = 30
benchmark_min_time = 0.1

@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
def temp_dir() -> Generator[Path, None, None]:
    """Provide a temporary directory for tests"""
    with tempfile.TemporaryDirectory() as temp_dir:
        yield Path(temp_dir)

@pytest.fixture
def sample_network_data() -> Dict[str, Any]:
    """Generate sample 5G network data for testing"""
    return {
        "network_id": "test-network-001",
        "timestamp": "2024-01-15T10:30:00Z",
        "base_stations": [
            {
                "id": "bs-001",
                "location": {"lat": 37.7749, "lon": -122.4194},
                "frequency_band": "3.5GHz",
                "power_level": 40,
                "coverage_radius": 1000,
                "active_users": 150,
                "throughput": 850.5,
                "latency": 2.1
            },
            {
                "id": "bs-002", 
                "location": {"lat": 37.7849, "lon": -122.4094},
                "frequency_band": "28GHz",
                "power_level": 35,
                "coverage_radius": 300,
                "active_users": 75,
                "throughput": 1200.8,
                "latency": 1.8
            }
        ],
        "network_slices": [
            {
                "slice_id": "embb-001",
                "type": "eMBB",
                "bandwidth": 100,
                "latency_requirement": 20,
                "throughput_requirement": 1000
            },
            {
                "slice_id": "urllc-001", 
                "type": "URLLC",
                "bandwidth": 20,
                "latency_requirement": 1,
                "throughput_requirement": 100
            }
        ],
        "kpis": {
            "total_throughput": 2051.3,
            "average_latency": 1.95,
            "packet_loss": 0.001,
            "availability": 99.99,
            "energy_efficiency": 85.6
        }
    }

@pytest.fixture
def sample_optimization_request() -> Dict[str, Any]:
    """Generate sample optimization request for testing"""
    return {
        "network_id": "test-network-001",
        "parameters": {
            "bandwidth": 1000,
            "latency_target": 1,
            "coverage_area": 50,
            "user_density": 1000
        },
        "constraints": {
            "power_budget": 500,
            "regulatory_limits": True
        },
        "objectives": [
            "maximize_throughput",
            "minimize_latency", 
            "optimize_energy"
        ]
    }

@pytest.fixture
def sample_time_series_data() -> pd.DataFrame:
    """Generate sample time series data for predictive testing"""
    dates = pd.date_range("2024-01-01", periods=1000, freq="H")
    np.random.seed(42)
    
    # Generate realistic network metrics with trends and seasonality
    base_throughput = 1000
    trend = np.linspace(0, 200, 1000)
    seasonal = 100 * np.sin(2 * np.pi * np.arange(1000) / 168)  # Weekly pattern
    noise = np.random.normal(0, 50, 1000)
    throughput = base_throughput + trend + seasonal + noise
    
    base_latency = 5
    latency_noise = np.random.normal(0, 0.5, 1000)
    latency = base_latency + latency_noise
    latency = np.maximum(latency, 0.1)  # Ensure positive latency
    
    return pd.DataFrame({
        "timestamp": dates,
        "throughput": throughput,
        "latency": latency,
        "packet_loss": np.random.uniform(0, 0.01, 1000),
        "active_users": np.random.poisson(1000, 1000),
        "energy_consumption": np.random.uniform(400, 600, 1000)
    })

@pytest.fixture
def mock_azure_services():
    """Mock Azure services for testing"""
    return {
        "cosmos_client": Mock(),
        "redis_client": Mock(),
        "openai_client": Mock(),
        "monitor_client": Mock(),
        "key_vault_client": Mock()
    }

@pytest.fixture
async def async_mock_services():
    """Async mock services for testing"""
    return {
        "async_db": AsyncMock(),
        "async_cache": AsyncMock(), 
        "async_ai_service": AsyncMock()
    }

@pytest.fixture
def cognitive_engine_config() -> Dict[str, Any]:
    """Configuration for cognitive intelligence engine testing"""
    return {
        "quantum_optimization": {
            "enabled": True,
            "algorithm": "QAOA",
            "max_iterations": 100,
            "convergence_threshold": 1e-6
        },
        "neuromorphic_processing": {
            "enabled": True,
            "spike_threshold": 0.7,
            "learning_rate": 0.01,
            "neuron_count": 1000
        },
        "digital_twin": {
            "enabled": True,
            "update_frequency": "real-time",
            "fidelity_threshold": 0.95
        }
    }

@pytest.fixture
def edge_ai_config() -> Dict[str, Any]:
    """Configuration for edge AI testing"""
    return {
        "inference_engine": {
            "framework": "onnx",
            "optimization_level": "aggressive",
            "latency_target": 1,  # milliseconds
            "batch_size": 1
        },
        "federated_learning": {
            "enabled": True,
            "privacy_budget": 1.0,
            "aggregation_method": "fedavg",
            "differential_privacy": True
        },
        "model_compression": {
            "quantization": "int8",
            "pruning_ratio": 0.5,
            "knowledge_distillation": True
        }
    }

@pytest.fixture
def security_ai_config() -> Dict[str, Any]:
    """Configuration for security AI testing"""
    return {
        "threat_detection": {
            "sensitivity": "high",
            "response_time_target": 5,  # seconds
            "false_positive_threshold": 0.01
        },
        "anomaly_detection": {
            "algorithm": "isolation_forest",
            "contamination": 0.1,
            "window_size": 100
        },
        "incident_response": {
            "auto_mitigation": True,
            "escalation_threshold": "high",
            "notification_channels": ["email", "slack"]
        }
    }

@pytest.fixture
def performance_metrics():
    """Performance benchmarking fixtures"""
    return {
        "latency_targets": {
            "optimization": 200,  # milliseconds
            "inference": 1,      # milliseconds  
            "threat_detection": 5000,  # milliseconds
            "prediction": 10000  # milliseconds
        },
        "throughput_targets": {
            "api_requests": 1000,  # requests/second
            "data_processing": 10000,  # records/second
            "model_inference": 10000   # inferences/second
        },
        "accuracy_targets": {
            "optimization_confidence": 0.85,
            "prediction_accuracy": 0.90,
            "threat_detection_precision": 0.95,
            "threat_detection_recall": 0.90
        }
    }

@pytest.fixture
def database_fixtures(tmp_path):
    """Database fixtures for testing"""
    # Create temporary SQLite database for testing
    db_path = tmp_path / "test.db"
    return {
        "db_url": f"sqlite:///{db_path}",
        "test_data": {
            "networks": [
                {"id": "net-001", "name": "Test Network 1"},
                {"id": "net-002", "name": "Test Network 2"}
            ],
            "optimizations": [
                {"id": "opt-001", "network_id": "net-001", "status": "completed"},
                {"id": "opt-002", "network_id": "net-002", "status": "running"}
            ]
        }
    }

# Custom pytest hooks
def pytest_runtest_setup(item):
    """Setup for each test item"""
    # Skip slow tests unless explicitly requested
    if "slow" in item.keywords and not item.config.getoption("--run-slow"):
        pytest.skip("Skipping slow test (use --run-slow to run)")
    
    # Skip Azure tests unless Azure credentials are available
    if "azure" in item.keywords and not os.getenv("AZURE_SUBSCRIPTION_ID"):
        pytest.skip("Skipping Azure test (no Azure credentials)")

def pytest_addoption(parser):
    """Add custom command line options"""
    parser.addoption(
        "--run-slow",
        action="store_true", 
        default=False,
        help="Run slow tests"
    )
    parser.addoption(
        "--run-azure",
        action="store_true",
        default=False, 
        help="Run Azure integration tests"
    )
    parser.addoption(
        "--benchmark",
        action="store_true",
        default=False,
        help="Run performance benchmarks"
    )

def pytest_collection_modifyitems(config, items):
    """Modify test collection"""
    # Add slow marker to tests that take more than 10 seconds
    slow_marker = pytest.mark.slow
    azure_marker = pytest.mark.azure
    
    for item in items:
        # Mark tests in certain modules as slow
        if "performance" in str(item.fspath) or "benchmark" in str(item.fspath):
            item.add_marker(slow_marker)
        
        # Mark Azure integration tests
        if "azure" in str(item.fspath) or "cloud" in str(item.fspath):
            item.add_marker(azure_marker)

@pytest.fixture(autouse=True)
def reset_environment():
    """Reset environment variables for each test"""
    original_env = os.environ.copy()
    yield
    os.environ.clear()
    os.environ.update(original_env)

# Performance testing utilities
class PerformanceValidator:
    """Utility class for validating performance requirements"""
    
    @staticmethod
    def validate_latency(actual_time: float, target_time: float, tolerance: float = 0.1):
        """Validate that actual latency meets target with tolerance"""
        max_allowed = target_time * (1 + tolerance)
        assert actual_time <= max_allowed, (
            f"Latency {actual_time}ms exceeds target {target_time}ms "
            f"(max allowed: {max_allowed}ms)"
        )
    
    @staticmethod
    def validate_throughput(actual_rate: float, target_rate: float, tolerance: float = 0.1):
        """Validate that actual throughput meets target with tolerance"""
        min_allowed = target_rate * (1 - tolerance)
        assert actual_rate >= min_allowed, (
            f"Throughput {actual_rate}/s below target {target_rate}/s "
            f"(min allowed: {min_allowed}/s)"
        )
    
    @staticmethod 
    def validate_accuracy(actual_score: float, target_score: float, tolerance: float = 0.05):
        """Validate that actual accuracy meets target with tolerance"""
        min_allowed = target_score - tolerance
        assert actual_score >= min_allowed, (
            f"Accuracy {actual_score} below target {target_score} "
            f"(min allowed: {min_allowed})"
        )

@pytest.fixture
def performance_validator():
    """Provide performance validation utilities"""
    return PerformanceValidator()

# Test data generators
class TestDataGenerator:
    """Generate various types of test data"""
    
    @staticmethod
    def generate_network_topology(num_nodes: int = 10) -> Dict[str, Any]:
        """Generate random network topology"""
        np.random.seed(42)
        nodes = []
        edges = []
        
        for i in range(num_nodes):
            nodes.append({
                "id": f"node-{i:03d}",
                "type": "base_station",
                "location": {
                    "lat": np.random.uniform(37.7, 37.8),
                    "lon": np.random.uniform(-122.5, -122.4)
                },
                "capacity": np.random.uniform(500, 2000),
                "load": np.random.uniform(0.1, 0.9)
            })
        
        # Generate edges (connections between nodes)
        for i in range(num_nodes):
            for j in range(i + 1, num_nodes):
                if np.random.random() < 0.3:  # 30% connection probability
                    edges.append({
                        "source": f"node-{i:03d}",
                        "target": f"node-{j:03d}",
                        "bandwidth": np.random.uniform(100, 1000),
                        "latency": np.random.uniform(1, 10)
                    })
        
        return {"nodes": nodes, "edges": edges}
    
    @staticmethod
    def generate_user_behavior_data(num_users: int = 1000) -> pd.DataFrame:
        """Generate synthetic user behavior data"""
        np.random.seed(42)
        
        user_data = []
        for i in range(num_users):
            user_data.append({
                "user_id": f"user-{i:06d}",
                "device_type": np.random.choice(["smartphone", "tablet", "iot"]),
                "data_usage": np.random.exponential(1000),  # MB
                "session_duration": np.random.gamma(2, 30),  # minutes
                "mobility_pattern": np.random.choice(["stationary", "pedestrian", "vehicular"]),
                "service_type": np.random.choice(["video", "gaming", "browsing", "voip"])
            })
        
        return pd.DataFrame(user_data)

@pytest.fixture
def test_data_generator():
    """Provide test data generation utilities"""
    return TestDataGenerator()
